"""
RAG ì‹œìŠ¤í…œì„ ìœ„í•œ ë°ì´í„° ë¶„ì„ ëª¨ë“ˆ
í¬ë¡¤ë§ëœ FAQ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
"""

import json
import pandas as pd
from collections import Counter, defaultdict
import re
from pathlib import Path
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

class FAQDataAnalyzer:
    """FAQ ë°ì´í„° ë¶„ì„ í´ë˜ìŠ¤"""
    
    def __init__(self, data_path: str):
        """
        Args:
            data_path: JSON íŒŒì¼ ê²½ë¡œ
        """
        self.data_path = Path(data_path)
        self.data = None
        self.df = None
        self.categories = {}
        
    def load_data(self) -> pd.DataFrame:
        """JSON ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  DataFrameìœ¼ë¡œ ë³€í™˜"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                self.data = json.load(f)
            
            self.df = pd.DataFrame(self.data)
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.df)}ê°œ FAQ")
            return self.df
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def analyze_structure(self) -> Dict:
        """ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        if self.df is None:
            print("âŒ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return {}
        
        analysis = {
            'total_count': len(self.df),
            'columns': list(self.df.columns),
            'data_types': dict(self.df.dtypes),
            'missing_values': dict(self.df.isnull().sum()),
            'sample_record': self.df.iloc[0].to_dict() if len(self.df) > 0 else {}
        }
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
        if 'title' in self.df.columns:
            analysis['title_length'] = {
                'mean': self.df['title'].str.len().mean(),
                'max': self.df['title'].str.len().max(),
                'min': self.df['title'].str.len().min()
            }
        
        if 'content' in self.df.columns:
            analysis['content_length'] = {
                'mean': self.df['content'].str.len().mean(),
                'max': self.df['content'].str.len().max(),
                'min': self.df['content'].str.len().min()
            }
        
        return analysis
    
    def categorize_faqs(self) -> Dict[str, List]:
        """FAQë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜"""
        if self.df is None:
            return {}
        
        # URL ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        url_categories = defaultdict(list)
        for idx, row in self.df.iterrows():
            url = row.get('url', '')
            if 'depth_1=' in url and 'depth_2=' in url:
                # URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì½”ë“œ ì¶”ì¶œ
                depth1_match = re.search(r'depth_1=([A-Z0-9]+)', url)
                depth2_match = re.search(r'depth_2=([A-Z0-9]+)', url)
                
                if depth1_match and depth2_match:
                    category = f"{depth1_match.group(1)}_{depth2_match.group(1)}"
                    url_categories[category].append(idx)
        
        # ì œëª© ê¸°ë°˜ í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
        keyword_categories = {
            'ì£¼íƒì§€ì›': ['ì£¼íƒ', 'ë‹¨ë…ì£¼íƒ', 'ê³µë™ì£¼íƒ', 'ì•„íŒŒíŠ¸'],
            'ê±´ë¬¼ì§€ì›': ['ê±´ë¬¼', 'ì‹œì„¤ë¬¼', 'ìƒì—…', 'ê³µì¥'],
            'ê¸ˆìœµì§€ì›': ['ê¸ˆìœµ', 'ìœµì', 'ëŒ€ì¶œ', 'ì§€ì›ê¸ˆ', 'ë³´ì¡°ê¸ˆ'],
            'RPSì œë„': ['RPS', 'ê³µê¸‰ì¸ì¦ì„œ', 'REC', 'ì˜ë¬´í™”'],
            'KSì¸ì¦': ['KSì¸ì¦', 'ì¸ì¦', 'ê²€ì¦', 'ì‹œí—˜'],
            'íƒœì–‘ê´‘': ['íƒœì–‘ê´‘', 'íƒœì–‘ì—´', 'ëª¨ë“ˆ', 'ì¸ë²„í„°'],
            'í’ë ¥': ['í’ë ¥', 'í’ì°¨'],
            'ì§€ì—´': ['ì§€ì—´'],
            'ì—°ë£Œì „ì§€': ['ì—°ë£Œì „ì§€'],
            'ESS': ['ESS', 'EMS', 'ì—ë„ˆì§€ì €ì¥'],
            'íƒ„ì†Œê²€ì¦': ['íƒ„ì†Œ', 'ë°°ì¶œëŸ‰', 'ê²€ì¦']
        }
        
        keyword_classification = defaultdict(list)
        for idx, row in self.df.iterrows():
            title = row.get('title', '').lower()
            content = row.get('content', '').lower()
            text = f"{title} {content}"
            
            for category, keywords in keyword_categories.items():
                if any(keyword in text for keyword in keywords):
                    keyword_classification[category].append(idx)
        
        self.categories = {
            'url_based': dict(url_categories),
            'keyword_based': dict(keyword_classification)
        }
        
        return self.categories
    
    def extract_statistics(self) -> Dict:
        """ë°ì´í„° í†µê³„ ì •ë³´ ì¶”ì¶œ"""
        if self.df is None:
            return {}
        
        stats = {}
        
        # ê¸°ë³¸ í†µê³„
        stats['basic'] = {
            'total_faqs': len(self.df),
            'unique_pages': self.df['page'].nunique() if 'page' in self.df.columns else 0,
            'crawl_date': self.df['crawled_at'].iloc[0] if 'crawled_at' in self.df.columns else None
        }
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        if self.categories:
            stats['categories'] = {}
            for cat_type, categories in self.categories.items():
                stats['categories'][cat_type] = {
                    cat: len(indices) for cat, indices in categories.items()
                }
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬
        if 'title' in self.df.columns and 'content' in self.df.columns:
            stats['text_length'] = {
                'title': {
                    'mean': round(self.df['title'].str.len().mean(), 2),
                    'std': round(self.df['title'].str.len().std(), 2),
                    'median': self.df['title'].str.len().median()
                },
                'content': {
                    'mean': round(self.df['content'].str.len().mean(), 2),
                    'std': round(self.df['content'].str.len().std(), 2),
                    'median': self.df['content'].str.len().median()
                }
            }
        
        return stats
    
    def detect_duplicates(self) -> Dict:
        """ì¤‘ë³µ ë°ì´í„° íƒì§€"""
        if self.df is None:
            return {}
        
        duplicates = {}
        
        # ì œëª© ê¸°ë°˜ ì¤‘ë³µ
        if 'title' in self.df.columns:
            title_duplicates = self.df[self.df.duplicated('title', keep=False)]
            duplicates['title'] = {
                'count': len(title_duplicates),
                'examples': title_duplicates['title'].unique()[:5].tolist()
            }
        
        # URL ê¸°ë°˜ ì¤‘ë³µ
        if 'url' in self.df.columns:
            url_duplicates = self.df[self.df.duplicated('url', keep=False)]
            duplicates['url'] = {
                'count': len(url_duplicates),
                'examples': url_duplicates['url'].unique()[:5].tolist()
            }
        
        # ë‚´ìš© ê¸°ë°˜ ì¤‘ë³µ (í•´ì‹œ ê¸°ë°˜)
        if 'content' in self.df.columns:
            content_hash = self.df['content'].str.len()  # ê°„ë‹¨í•œ ê¸¸ì´ ê¸°ë°˜ ë¹„êµ
            content_duplicates = self.df[content_hash.duplicated(keep=False)]
            duplicates['content_length'] = {
                'count': len(content_duplicates),
                'note': 'ë‚´ìš© ê¸¸ì´ ê¸°ë°˜ ì¤‘ë³µ ì¶”ì •'
            }
        
        return duplicates
    
    def generate_report(self, output_path: str = None) -> str:
        """ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        if output_path is None:
            output_path = f"faq_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        # ë¶„ì„ ì‹¤í–‰
        structure = self.analyze_structure()
        categories = self.categorize_faqs()
        stats = self.extract_statistics()
        duplicates = self.detect_duplicates()
        
        # ë³´ê³ ì„œ ì‘ì„±
        report = []
        report.append("=" * 60)
        report.append("FAQ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ")
        report.append("=" * 60)
        report.append(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"ë°ì´í„° íŒŒì¼: {self.data_path}")
        report.append("")
        
        # 1. ê¸°ë³¸ êµ¬ì¡°
        report.append("1. ë°ì´í„° êµ¬ì¡° ë¶„ì„")
        report.append("-" * 30)
        report.append(f"ì´ FAQ ìˆ˜: {structure.get('total_count', 0)}")
        report.append(f"ì»¬ëŸ¼: {', '.join(structure.get('columns', []))}")
        report.append("")
        
        if 'title_length' in structure:
            tl = structure['title_length']
            report.append(f"ì œëª© ê¸¸ì´ - í‰ê· : {tl['mean']:.1f}, ìµœëŒ€: {tl['max']}, ìµœì†Œ: {tl['min']}")
        
        if 'content_length' in structure:
            cl = structure['content_length']
            report.append(f"ë‚´ìš© ê¸¸ì´ - í‰ê· : {cl['mean']:.1f}, ìµœëŒ€: {cl['max']}, ìµœì†Œ: {cl['min']}")
        
        report.append("")
        
        # 2. ì¹´í…Œê³ ë¦¬ ë¶„ì„
        report.append("2. ì¹´í…Œê³ ë¦¬ ë¶„ì„")
        report.append("-" * 30)
        if 'keyword_based' in self.categories:
            for cat, indices in self.categories['keyword_based'].items():
                report.append(f"{cat}: {len(indices)}ê°œ")
        report.append("")
        
        # 3. ì¤‘ë³µ ë°ì´í„°
        report.append("3. ì¤‘ë³µ ë°ì´í„° ë¶„ì„")
        report.append("-" * 30)
        for dup_type, dup_info in duplicates.items():
            report.append(f"{dup_type} ì¤‘ë³µ: {dup_info.get('count', 0)}ê°œ")
        report.append("")
        
        # 4. ìƒ˜í”Œ ë°ì´í„°
        report.append("4. ìƒ˜í”Œ ë°ì´í„°")
        report.append("-" * 30)
        if len(self.df) > 0:
            sample = self.df.iloc[0]
            report.append(f"ì œëª©: {sample.get('title', 'N/A')}")
            report.append(f"ë‚´ìš©: {sample.get('content', 'N/A')[:100]}...")
            report.append(f"URL: {sample.get('url', 'N/A')}")
        
        report_text = "\n".join(report)
        
        # íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… ë¶„ì„ ë³´ê³ ì„œ ìƒì„±: {output_path}")
        return report_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ” FAQ ë°ì´í„° ë¶„ì„ ì‹œì‘")
    
    # ë°ì´í„° íŒŒì¼ ê²½ë¡œ (ìˆ˜ì • í•„ìš”)
    data_file = "crawler/output/data/knrec_faq_20250611_181612.json"
    
    if not Path(data_file).exists():
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_file}")
        print("ì˜¬ë°”ë¥¸ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = FAQDataAnalyzer(data_file)
    
    # ë°ì´í„° ë¡œë“œ
    df = analyzer.load_data()
    if df is None:
        return
    
    # ë¶„ì„ ì‹¤í–‰ ë° ë³´ê³ ì„œ ìƒì„±
    report = analyzer.generate_report()
    print("\n" + "="*50)
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("="*50)
    print(report)


if __name__ == "__main__":
    main() 