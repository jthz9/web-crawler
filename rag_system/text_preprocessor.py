"""
RAG ì‹œìŠ¤í…œì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ
í•œêµ­ì–´ FAQ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
"""

import re
import json
import pandas as pd
from typing import List, Dict, Tuple
from pathlib import Path
import html
from datetime import datetime

class KoreanTextPreprocessor:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
        self.stopwords = {
            'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë”°ë¼ì„œ', 'ì¦‰', 'ì˜ˆë¥¼ ë“¤ì–´',
            'ì…ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ê²ƒì…ë‹ˆë‹¤',
            'ì´ëŠ”', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°',
            'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 'ì´ë ‡ê²Œ', 'ê·¸ë ‡ê²Œ', 'ì €ë ‡ê²Œ',
            'ë§¤ìš°', 'ì •ë§', 'ë„ˆë¬´', 'ìƒë‹¹íˆ', 'ê½¤', 'ì•„ì£¼',
            'ë“±ë“±', 'ê¸°íƒ€', 'ì™¸ì—', 'ì´ì™¸', 'ì œì™¸í•˜ê³ '
        }
        
        # HTML ì—”í‹°í‹° ë§¤í•‘
        self.html_entities = {
            '&nbsp;': ' ',
            '&amp;': '&',
            '&lt;': '<',
            '&gt;': '>',
            '&quot;': '"',
            '&#39;': "'",
            '&middot;': 'Â·',
            '&bull;': 'â€¢'
        }
    
    def clean_html(self, text: str) -> str:
        """HTML íƒœê·¸ ë° ì—”í‹°í‹° ì œê±°"""
        if not text:
            return ""
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        
        # HTML ì—”í‹°í‹° ë³€í™˜
        text = html.unescape(text)
        for entity, replacement in self.html_entities.items():
            text = text.replace(entity, replacement)
        
        return text
    
    def normalize_whitespace(self, text: str) -> str:
        """ê³µë°± ì •ê·œí™”"""
        if not text:
            return ""
        
        # ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        text = text.strip()
        
        # ë¬¸ì¥ ë ì •ë¦¬
        text = re.sub(r'[.]{2,}', '.', text)
        text = re.sub(r'[?]{2,}', '?', text)
        text = re.sub(r'[!]{2,}', '!', text)
        
        return text
    
    def clean_special_chars(self, text: str) -> str:
        """íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬"""
        if not text:
            return ""
        
        # ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ ìœ ì§€)
        text = re.sub(r'[^\w\sê°€-í£.,!?()[\]{}:;"\'-]', ' ', text)
        
        # ì—°ì†ëœ êµ¬ë‘ì  ì •ë¦¬
        text = re.sub(r'[.,!?]{3,}', '...', text)
        
        return text
    
    def extract_keywords(self, text: str) -> List[str]:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (í˜•íƒœì†Œ ë¶„ì„ ì—†ì´)"""
        if not text:
            return []
        
        # 2ê¸€ì ì´ìƒì˜ í•œê¸€ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        keywords = [word for word in words if word not in self.stopwords]
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ìˆœ ì •ë ¬
        from collections import Counter
        word_counts = Counter(keywords)
        
        return [word for word, count in word_counts.most_common(10)]
    
    def chunk_text(self, text: str, max_length: int = 300) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        if not text or len(text) <= max_length:
            return [text] if text else []
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€í•´ë„ ê¸¸ì´ ì œí•œì„ ë„˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
            if len(current_chunk + sentence) <= max_length:
                current_chunk += sentence + ". "
            else:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_faq(self, faq_item: Dict) -> Dict:
        """ë‹¨ì¼ FAQ ì•„ì´í…œ ì „ì²˜ë¦¬"""
        processed = faq_item.copy()
        
        # ì œëª© ì „ì²˜ë¦¬
        if 'title' in processed:
            title = processed['title']
            title = self.clean_html(title)
            title = self.normalize_whitespace(title)
            title = self.clean_special_chars(title)
            processed['title_cleaned'] = title
        
        # ë‚´ìš© ì „ì²˜ë¦¬
        if 'content' in processed:
            content = processed['content']
            content = self.clean_html(content)
            content = self.normalize_whitespace(content)
            content = self.clean_special_chars(content)
            processed['content_cleaned'] = content
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            processed['keywords'] = self.extract_keywords(content)
            
            # í…ìŠ¤íŠ¸ ì²­í‚¹
            processed['chunks'] = self.chunk_text(content)
            processed['chunk_count'] = len(processed['chunks'])
        
        # ê²°í•©ëœ í…ìŠ¤íŠ¸ (ì œëª© + ë‚´ìš©)
        combined_text = ""
        if 'title_cleaned' in processed:
            combined_text += processed['title_cleaned'] + " "
        if 'content_cleaned' in processed:
            combined_text += processed['content_cleaned']
        
        processed['combined_text'] = combined_text.strip()
        processed['text_length'] = len(combined_text)
        
        # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        processed['preprocessed_at'] = datetime.now().isoformat()
        
        return processed

class FAQPreprocessor:
    """FAQ ë°ì´í„°ì…‹ ì „ì²´ ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.text_processor = KoreanTextPreprocessor()
        self.stats = {}
    
    def load_data(self, file_path: str) -> List[Dict]:
        """FAQ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(data)}ê°œ FAQ")
            return data
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def preprocess_dataset(self, data: List[Dict]) -> List[Dict]:
        """ì „ì²´ ë°ì´í„°ì…‹ ì „ì²˜ë¦¬"""
        print("ğŸ”„ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘...")
        
        processed_data = []
        original_lengths = []
        processed_lengths = []
        
        for i, faq in enumerate(data):
            if i % 50 == 0:
                print(f"ì§„í–‰ë¥ : {i}/{len(data)} ({i/len(data)*100:.1f}%)")
            
            # ì›ë³¸ ê¸¸ì´ ê¸°ë¡
            original_text = (faq.get('title', '') + ' ' + faq.get('content', '')).strip()
            original_lengths.append(len(original_text))
            
            # ì „ì²˜ë¦¬ ìˆ˜í–‰
            processed_faq = self.text_processor.process_faq(faq)
            processed_data.append(processed_faq)
            
            # ì²˜ë¦¬ í›„ ê¸¸ì´ ê¸°ë¡
            processed_lengths.append(processed_faq.get('text_length', 0))
        
        # í†µê³„ ê³„ì‚°
        self.stats = {
            'total_count': len(processed_data),
            'original_avg_length': sum(original_lengths) / len(original_lengths) if original_lengths else 0,
            'processed_avg_length': sum(processed_lengths) / len(processed_lengths) if processed_lengths else 0,
            'total_chunks': sum(faq.get('chunk_count', 0) for faq in processed_data),
            'avg_chunks_per_faq': sum(faq.get('chunk_count', 0) for faq in processed_data) / len(processed_data) if processed_data else 0
        }
        
        print("âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ!")
        return processed_data
    
    def remove_duplicates(self, data: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ì œê±°"""
        print("ğŸ”„ ì¤‘ë³µ ì œê±° ì‹œì‘...")
        
        seen_texts = set()
        unique_data = []
        duplicate_count = 0
        
        for faq in data:
            combined_text = faq.get('combined_text', '')
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ì™€ ì²« 50ìë¡œ ê°„ë‹¨í•œ ì¤‘ë³µ ê²€ì‚¬
            text_signature = f"{len(combined_text)}_{combined_text[:50]}"
            
            if text_signature not in seen_texts:
                seen_texts.add(text_signature)
                unique_data.append(faq)
            else:
                duplicate_count += 1
        
        print(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {duplicate_count}ê°œ ì¤‘ë³µ ì œê±°, {len(unique_data)}ê°œ ìœ ì§€")
        return unique_data
    
    def save_processed_data(self, data: List[Dict], output_path: str) -> str:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")
            return output_path
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return ""
    
    def generate_preprocessing_report(self, output_path: str = None) -> str:
        """ì „ì²˜ë¦¬ ë³´ê³ ì„œ ìƒì„±"""
        if output_path is None:
            output_path = f"preprocessing_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        report = []
        report.append("=" * 60)
        report.append("í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë³´ê³ ì„œ")
        report.append("=" * 60)
        report.append(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        if self.stats:
            report.append("ğŸ“Š ì „ì²˜ë¦¬ í†µê³„")
            report.append("-" * 30)
            report.append(f"ì´ FAQ ìˆ˜: {self.stats['total_count']}")
            report.append(f"ì›ë³¸ í‰ê·  ê¸¸ì´: {self.stats['original_avg_length']:.1f}ì")
            report.append(f"ì²˜ë¦¬ í›„ í‰ê·  ê¸¸ì´: {self.stats['processed_avg_length']:.1f}ì")
            report.append(f"ì´ ì²­í¬ ìˆ˜: {self.stats['total_chunks']}")
            report.append(f"FAQë‹¹ í‰ê·  ì²­í¬ ìˆ˜: {self.stats['avg_chunks_per_faq']:.1f}ê°œ")
        
        report_text = "\n".join(report)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"âœ… ì „ì²˜ë¦¬ ë³´ê³ ì„œ ìƒì„±: {output_path}")
        return report_text


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”„ FAQ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‹œì‘")
    
    # ì…ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    input_file = "crawler/output/data/knrec_faq_20250611_181612.json"
    output_file = f"rag_system/processed_faq_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    if not Path(input_file).exists():
        print(f"âŒ ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        return
    
    # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
    preprocessor = FAQPreprocessor()
    
    # ë°ì´í„° ë¡œë“œ
    data = preprocessor.load_data(input_file)
    if not data:
        return
    
    # ì „ì²˜ë¦¬ ìˆ˜í–‰
    processed_data = preprocessor.preprocess_dataset(data)
    
    # ì¤‘ë³µ ì œê±°
    unique_data = preprocessor.remove_duplicates(processed_data)
    
    # ì €ì¥
    preprocessor.save_processed_data(unique_data, output_file)
    
    # ë³´ê³ ì„œ ìƒì„±
    report = preprocessor.generate_preprocessing_report()
    print("\n" + "="*50)
    print("ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("="*50)
    print(report)


if __name__ == "__main__":
    main() 